---
title: "ETFproject"
output: html_document
---


https://docs.docker.com/docker-for-mac/install/#download-docker-for-mac


"docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.0"

http://localhost/

"ps docker"

despues de instalar y correr docker, lo unico que tenemos que hacer es paginar
las 20 paginas
system("docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.0")



```{r  scrapping, echo=FALSE}
rm(list = ls())
library('RSelenium')
library('XML')

remDr <- remoteDriver(port = 4445L)
remDr$open()

df <- data.frame(Date=as.Date(character()),
		File=character(), 
		User=character(), 
		stringsAsFactors=FALSE) 


for (i in 1:20) {  
	
	url = paste0("http://etfdb.com/type/sector/all/#etfs__overview&sort_name=assets_under_management&sort_order=desc&page=",i)
	remDr$open()
	remDr$navigate(url)
	
 html <- unlist(remDr$getPageSource())
 
 table <- readHTMLTable(html)[[2]]
 
 df <-rbind( df ,table )
 
}


 saveRDS(df, file.path(getwd(), "df.rds"))
 
 

```
at this point better if we stop Stop / remove all Docker containers to free up some resources in our system. 

docker stop $(docker ps -a -q) 
docker rm $(docker ps -a -q)



```{r  data_munging, echo=FALSE}
rm(list = ls())
getwd()

library(dplyr)
options(digits=2)

train <- readRDS("D:/repos/Scraping/df.rds") %>% as.data.frame() # SILENCE
class(train)


names(train) <- gsub("[\n]| ",'',names(train))   ## nicer names 
test <- train %>%    select(1,2,13)


summary(test)
str(test)


test[, c(1,2)] <- sapply(test[, c(1,2)], as.character)
test[, c(3)]<- as.numeric(sub("%", "", test[, c(3)]))

test[,order(3)]

percent_vec = paste(1:100, "%", sep = "")
as.numeric(sub("%", "", percent_vec))


library(plyr)
arrange(test,desc(3),1)

train[, c(3)] <- sapply(gsub("[$]", "", train[,3]), as.numeric)
     


median(train[,3])

train[, c(3)] <-as.numeric(sub('\\$','',as.character(train[, c(3)]))) 


class(train)


describe(train)


```

#la idea - book

 
```{r  ExploratoryAnalysis, echo=FALSE}
rm(list = ls())
 train <- readRDS("df.rds")
 

```
 
 